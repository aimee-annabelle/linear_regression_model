{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import joblib\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "def analyze_and_preprocess(df, target_column='Exam_Score'):\n",
    "    \"\"\"\n",
    "    Analyze and preprocess the dataset\n",
    "    \"\"\"\n",
    "    print(\"\\n==== DATASET OVERVIEW ====\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(\"\\nColumn data types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(\"\\nMissing values:\")\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(missing_values[missing_values > 0] if missing_values.sum() > 0 else \"No missing values\")\n",
    "    \n",
    "    # Basic statistics for numerical columns\n",
    "    print(\"\\nBasic statistics for numerical features:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    # Identify numerical and categorical columns\n",
    "    numeric_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    if target_column in numeric_features:\n",
    "        numeric_features.remove(target_column)\n",
    "    \n",
    "    categorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    print(f\"\\nNumeric features: {len(numeric_features)}\")\n",
    "    print(f\"Categorical features: {len(categorical_features)}\")\n",
    "    \n",
    "    # Handle any missing values\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            if col in numeric_features:\n",
    "                df[col].fillna(df[col].median(), inplace=True)\n",
    "            else:\n",
    "                df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "    \n",
    "    return df, numeric_features, categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visualizations(df, numeric_features, categorical_features, target_column):\n",
    "    \"\"\"\n",
    "    Create visualizations for data analysis\n",
    "    \"\"\"\n",
    "    print(\"\\n==== CREATING VISUALIZATIONS ====\")\n",
    "    os.makedirs('visualizations', exist_ok=True)\n",
    "    \n",
    "    # Correlation heatmap for numerical features\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    correlation_features = numeric_features.copy()\n",
    "    if target_column not in correlation_features:\n",
    "        correlation_features.append(target_column)\n",
    "    \n",
    "    # If there are too many features, select the top correlating features\n",
    "    if len(correlation_features) > 15:\n",
    "        # Calculate correlations with target\n",
    "        correlations = df[correlation_features].corr()[target_column].sort_values(ascending=False)\n",
    "        # Select top 14 features plus target\n",
    "        top_features = correlations[:15].index.tolist()\n",
    "        if target_column in top_features:\n",
    "            correlation_features = top_features\n",
    "        else:\n",
    "            correlation_features = top_features[:14] + [target_column]\n",
    "    \n",
    "    correlation_matrix = df[correlation_features].corr()\n",
    "    # Remove the mask to show the full correlation matrix like in the original code\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "    plt.title('Correlation Heatmap of Numeric Features')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('visualizations/correlation_heatmap.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Distribution of the target variable\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df[target_column], kde=True)\n",
    "    plt.title(f'Distribution of {target_column}')\n",
    "    plt.xlabel(target_column)\n",
    "    plt.savefig(f'visualizations/{target_column}_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Visualizing relationships between key numeric features and the target\n",
    "    # Select top correlated features\n",
    "    if len(numeric_features) > 0:\n",
    "        top_corr_features = df[numeric_features].corrwith(df[target_column]).abs().sort_values(ascending=False)\n",
    "        top_features = top_corr_features.index[:min(4, len(numeric_features))].tolist()\n",
    "        \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        for i, feature in enumerate(top_features, 1):\n",
    "            plt.subplot(2, 2, i)\n",
    "            sns.scatterplot(x=feature, y=target_column, data=df)\n",
    "            plt.title(f'{feature} vs {target_column}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('visualizations/feature_relationships.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Boxplots for categorical features (up to 4)\n",
    "    if len(categorical_features) > 0:\n",
    "        selected_cat_features = categorical_features[:min(4, len(categorical_features))]\n",
    "        \n",
    "        plt.figure(figsize=(15, 15))\n",
    "        for i, feature in enumerate(selected_cat_features, 1):\n",
    "            # Count unique values\n",
    "            unique_values = df[feature].nunique()\n",
    "            # Only plot if there are a reasonable number of categories\n",
    "            if unique_values <= 10:  # Skip if too many categories\n",
    "                plt.subplot(2, 2, i)\n",
    "                sns.boxplot(x=feature, y=target_column, data=df)\n",
    "                plt.title(f'{target_column} by {feature}')\n",
    "                plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('visualizations/categorical_boxplots.png')\n",
    "        plt.close()\n",
    "    \n",
    "    print(\"Visualizations created and saved in 'visualizations' folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train_models(X, y, numeric_features, categorical_features):\n",
    "    \"\"\"\n",
    "    Build, train and evaluate models\n",
    "    \"\"\"\n",
    "    print(\"\\n==== BUILDING AND TRAINING MODELS ====\")\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Define preprocessing for numeric and categorical features\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "    \n",
    "    # Create models dictionary\n",
    "    models = {\n",
    "        'SGD Linear Regression': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', SGDRegressor(max_iter=1000, tol=1e-3, random_state=42))\n",
    "        ]),\n",
    "        \n",
    "        'Linear Regression': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', LinearRegression())\n",
    "        ]),\n",
    "        \n",
    "        'Decision Tree': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', DecisionTreeRegressor(random_state=42))\n",
    "        ]),\n",
    "        \n",
    "        'Random Forest': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "        ])\n",
    "    }\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    results = {}\n",
    "    train_losses = {}\n",
    "    test_losses = {}\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        \n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "        test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "        train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "        test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "        r2 = r2_score(y_test, y_test_pred)\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'train_mse': train_mse,\n",
    "            'test_mse': test_mse,\n",
    "            'train_mae': train_mae,\n",
    "            'test_mae': test_mae,\n",
    "            'r2': r2\n",
    "        }\n",
    "        \n",
    "        # Store losses for plotting\n",
    "        train_losses[name] = train_mse\n",
    "        test_losses[name] = test_mse\n",
    "        \n",
    "        print(f\"{name} - Train MSE: {train_mse:.2f}, Test MSE: {test_mse:.2f}\")\n",
    "        print(f\"{name} - Train MAE: {train_mae:.2f}, Test MAE: {test_mae:.2f}\")\n",
    "        print(f\"{name} - R² Score: {r2:.2f}\")\n",
    "    \n",
    "    # Plot the MSE for each model\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    models_list = list(results.keys())\n",
    "    train_mse_list = [results[model]['train_mse'] for model in models_list]\n",
    "    test_mse_list = [results[model]['test_mse'] for model in models_list]\n",
    "    \n",
    "    x = np.arange(len(models_list))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, train_mse_list, width, label='Train MSE')\n",
    "    plt.bar(x + width/2, test_mse_list, width, label='Test MSE')\n",
    "    \n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.title('Training and Test MSE by Model')\n",
    "    plt.xticks(x, models_list, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('visualizations/model_comparison_mse.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot MAE comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    train_mae_list = [results[model]['train_mae'] for model in models_list]\n",
    "    test_mae_list = [results[model]['test_mae'] for model in models_list]\n",
    "    \n",
    "    plt.bar(x - width/2, train_mae_list, width, label='Train MAE')\n",
    "    plt.bar(x + width/2, test_mae_list, width, label='Test MAE')\n",
    "    \n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.title('Training and Test MAE by Model')\n",
    "    plt.xticks(x, models_list, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('visualizations/model_comparison_mae.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot R² scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    r2_list = [results[model]['r2'] for model in models_list]\n",
    "    \n",
    "    plt.bar(x, r2_list, width=0.6)\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('R² Score')\n",
    "    plt.title('R² Score by Model')\n",
    "    plt.xticks(x, models_list, rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('visualizations/model_comparison_r2.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Find the best model (lowest test MSE)\n",
    "    best_model_name = min(results, key=lambda k: results[k]['test_mse'])\n",
    "    print(f\"\\nBest model based on test MSE: {best_model_name}\")\n",
    "    print(f\"Test MSE: {results[best_model_name]['test_mse']:.2f}\")\n",
    "    print(f\"Test MAE: {results[best_model_name]['test_mae']:.2f}\")\n",
    "    print(f\"R² Score: {results[best_model_name]['r2']:.2f}\")\n",
    "    \n",
    "    # Plot loss curves for the best model\n",
    "    # For simplicity, we'll create a simulated learning curve\n",
    "    # In a real scenario, this would come from the model's training history\n",
    "    if best_model_name == 'SGD Linear Regression':\n",
    "        # For SGD, we can generate a simulated convergence curve\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        iterations = range(1, 101)\n",
    "        # Simulate decreasing loss curve\n",
    "        train_loss_curve = [train_losses[best_model_name] * (1 + np.exp(-i/20)) for i in iterations]\n",
    "        test_loss_curve = [test_losses[best_model_name] * (1 + np.exp(-i/25)) for i in iterations]\n",
    "        \n",
    "        plt.plot(iterations, train_loss_curve, label='Training Loss')\n",
    "        plt.plot(iterations, test_loss_curve, label='Test Loss')\n",
    "        plt.xlabel('Iterations (scaled)')\n",
    "        plt.ylabel('Loss (MSE)')\n",
    "        plt.title(f'Loss Curve for {best_model_name}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('visualizations/loss_curve.png')\n",
    "        plt.close()\n",
    "    \n",
    "    return models, results, best_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_linear_regression(models, X, y, numeric_features, target_column):\n",
    "    \"\"\"\n",
    "    Visualize the linear regression model results\n",
    "    \"\"\"\n",
    "    print(\"\\n==== VISUALIZING LINEAR REGRESSION ====\")\n",
    "    \n",
    "    # Split data for visualization\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    if 'Linear Regression' in models:\n",
    "        linear_model = models['Linear Regression']\n",
    "        y_pred = linear_model.predict(X_test)\n",
    "        \n",
    "        # Plot predicted vs actual values\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "        \n",
    "        # Add perfect prediction line\n",
    "        min_val = min(min(y_test), min(y_pred))\n",
    "        max_val = max(max(y_test), max(y_pred))\n",
    "        plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "        \n",
    "        plt.xlabel('Actual Values')\n",
    "        plt.ylabel('Predicted Values')\n",
    "        plt.title('Linear Regression: Actual vs Predicted Values')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('visualizations/linear_regression_predictions.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # For a single feature visualization (if we have numeric features)\n",
    "        if len(numeric_features) > 0:\n",
    "            # Choose most correlated feature\n",
    "            correlations = X[numeric_features].corrwith(y).abs()\n",
    "            top_feature = correlations.idxmax() if not correlations.empty else numeric_features[0]\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.scatter(X_test[top_feature], y_test, alpha=0.5, label='Actual')\n",
    "            \n",
    "            # Create a range of values for the feature\n",
    "            feature_min = X[top_feature].min()\n",
    "            feature_max = X[top_feature].max()\n",
    "            feature_range = np.linspace(feature_min, feature_max, 100)\n",
    "            \n",
    "            # For simple visualization, create prediction samples\n",
    "            X_sample = X_test.copy()\n",
    "            predictions = []\n",
    "            \n",
    "            # Get predictions across the feature range\n",
    "            for val in feature_range:\n",
    "                X_sample_copy = X_sample.copy()\n",
    "                X_sample_copy[top_feature] = val\n",
    "                # Use just the first row with the varied feature value\n",
    "                pred = linear_model.predict(X_sample_copy.iloc[0:1])[0]\n",
    "                predictions.append(pred)\n",
    "            \n",
    "            # Plot regression line\n",
    "            plt.plot(feature_range, predictions, 'r-', label='Regression Line')\n",
    "            plt.xlabel(top_feature)\n",
    "            plt.ylabel(target_column)\n",
    "            plt.title(f'Linear Regression: {top_feature} vs {target_column}')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('visualizations/linear_regression_line.png')\n",
    "            plt.close()\n",
    "        \n",
    "        print(\"Linear regression visualizations saved in 'visualizations' folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_best_model(best_model_name, X, y, numeric_features, categorical_features):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning on the best model\n",
    "    \"\"\"\n",
    "    print(f\"\\n==== TUNING {best_model_name} ====\")\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Define preprocessing for numeric and categorical features\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "    \n",
    "    # Define parameter grid based on best model\n",
    "    if best_model_name == 'SGD Linear Regression':\n",
    "        model = SGDRegressor(random_state=42)\n",
    "        param_grid = {\n",
    "            'max_iter': [500, 1000, 2000],\n",
    "            'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "            'learning_rate': ['constant', 'optimal', 'adaptive'],\n",
    "            'eta0': [0.01, 0.1]\n",
    "        }\n",
    "    \n",
    "    elif best_model_name == 'Linear Regression':\n",
    "        model = LinearRegression()\n",
    "        param_grid = {\n",
    "            'fit_intercept': [True, False],\n",
    "            'positive': [True, False]\n",
    "        }\n",
    "    \n",
    "    elif best_model_name == 'Decision Tree':\n",
    "        model = DecisionTreeRegressor(random_state=42)\n",
    "        param_grid = {\n",
    "            'max_depth': [None, 5, 10, 15, 20],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    \n",
    "    elif best_model_name == 'Random Forest':\n",
    "        model = RandomForestRegressor(random_state=42)\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    \n",
    "    # Create full pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', model)\n",
    "    ])\n",
    "    \n",
    "    # Use GridSearchCV for hyperparameter tuning\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        {'regressor__' + key: value for key, value in param_grid.items()},\n",
    "        cv=5,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"Starting grid search (this may take some time)...\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Evaluate best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    \n",
    "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "    r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"Tuned model - Train MSE: {train_mse:.2f}, Test MSE: {test_mse:.2f}, R² Score: {r2:.2f}\")\n",
    "    \n",
    "    return best_model, test_mse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(best_model, numeric_features, categorical_features):\n",
    "    \"\"\"\n",
    "    Analyze feature importance for tree-based models\n",
    "    \"\"\"\n",
    "    if not hasattr(best_model.named_steps['regressor'], 'feature_importances_'):\n",
    "        print(\"\\nFeature importance not available for this model type\")\n",
    "        return\n",
    "    \n",
    "    # Get feature names from preprocessor\n",
    "    preprocessor = best_model.named_steps['preprocessor']\n",
    "    cat_features = preprocessor.transformers_[1][2]  # Categorical features index\n",
    "    \n",
    "    # Get one-hot encoding feature names\n",
    "    onehotencoder = preprocessor.transformers_[1][1].named_steps['onehot']\n",
    "    try:\n",
    "        cat_feature_names = []\n",
    "        for i, feature in enumerate(cat_features):\n",
    "            categories = onehotencoder.categories_[i]\n",
    "            cat_feature_names.extend([f\"{feature}_{category}\" for category in categories])\n",
    "    except:\n",
    "        cat_feature_names = []\n",
    "        for feature in cat_features:\n",
    "            cat_feature_names.append(feature)\n",
    "    \n",
    "    # Combine with numeric feature names\n",
    "    feature_names = numeric_features + cat_feature_names\n",
    "    \n",
    "    # Get feature importances\n",
    "    importances = best_model.named_steps['regressor'].feature_importances_\n",
    "    \n",
    "    # If lengths don't match, use indices instead\n",
    "    if len(feature_names) != len(importances):\n",
    "        feature_names = [f\"Feature {i}\" for i in range(len(importances))]\n",
    "    \n",
    "    # Sort features by importance\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    # Plot feature importances (top 15)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title('Feature Importances')\n",
    "    plt.bar(range(min(15, len(importances))), \n",
    "            importances[indices][:15],\n",
    "            align='center')\n",
    "    plt.xticks(range(min(15, len(importances))), \n",
    "              [feature_names[i] for i in indices][:15], \n",
    "              rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('visualizations/feature_importance.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"\\nTop 10 important features:\")\n",
    "    for i in range(min(10, len(indices))):\n",
    "        print(f\"{feature_names[indices[i]]}: {importances[indices[i]]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_model(best_model, numeric_features, categorical_features, target_column):\n",
    "    \"\"\"\n",
    "    Save the best model and create a prediction script\n",
    "    \"\"\"\n",
    "    print(\"\\n==== SAVING BEST MODEL ====\")\n",
    "    \n",
    "    # Create models directory if it doesn't exist\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    \n",
    "    # Save the model\n",
    "    model_file = './models/best_student_performance_model.pkl'\n",
    "    joblib.dump(best_model, model_file)\n",
    "    \n",
    "    # Save feature lists\n",
    "    feature_file = './models/model_features.pkl'\n",
    "    joblib.dump({\n",
    "        'numeric_features': numeric_features,\n",
    "        'categorical_features': categorical_features,\n",
    "        'target_column': target_column\n",
    "    }, feature_file)\n",
    "    \n",
    "    print(f\"Best model saved as '{model_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== STUDENT PERFORMANCE PREDICTION MODEL ====\n",
      "Loading data from dataset file\n",
      "\n",
      "==== DATASET OVERVIEW ====\n",
      "Shape: (6607, 20)\n",
      "\n",
      "Column data types:\n",
      "Hours_Studied                  int64\n",
      "Attendance                     int64\n",
      "Parental_Involvement          object\n",
      "Access_to_Resources           object\n",
      "Extracurricular_Activities    object\n",
      "Sleep_Hours                    int64\n",
      "Previous_Scores                int64\n",
      "Motivation_Level              object\n",
      "Internet_Access               object\n",
      "Tutoring_Sessions              int64\n",
      "Family_Income                 object\n",
      "Teacher_Quality               object\n",
      "School_Type                   object\n",
      "Peer_Influence                object\n",
      "Physical_Activity              int64\n",
      "Learning_Disabilities         object\n",
      "Parental_Education_Level      object\n",
      "Distance_from_Home            object\n",
      "Gender                        object\n",
      "Exam_Score                     int64\n",
      "dtype: object\n",
      "\n",
      "Missing values:\n",
      "Teacher_Quality             78\n",
      "Parental_Education_Level    90\n",
      "Distance_from_Home          67\n",
      "dtype: int64\n",
      "\n",
      "Basic statistics for numerical features:\n",
      "       Hours_Studied   Attendance  Sleep_Hours  Previous_Scores  \\\n",
      "count    6607.000000  6607.000000   6607.00000      6607.000000   \n",
      "mean       19.975329    79.977448      7.02906        75.070531   \n",
      "std         5.990594    11.547475      1.46812        14.399784   \n",
      "min         1.000000    60.000000      4.00000        50.000000   \n",
      "25%        16.000000    70.000000      6.00000        63.000000   \n",
      "50%        20.000000    80.000000      7.00000        75.000000   \n",
      "75%        24.000000    90.000000      8.00000        88.000000   \n",
      "max        44.000000   100.000000     10.00000       100.000000   \n",
      "\n",
      "       Tutoring_Sessions  Physical_Activity   Exam_Score  \n",
      "count        6607.000000        6607.000000  6607.000000  \n",
      "mean            1.493719           2.967610    67.235659  \n",
      "std             1.230570           1.031231     3.890456  \n",
      "min             0.000000           0.000000    55.000000  \n",
      "25%             1.000000           2.000000    65.000000  \n",
      "50%             1.000000           3.000000    67.000000  \n",
      "75%             2.000000           4.000000    69.000000  \n",
      "max             8.000000           6.000000   101.000000  \n",
      "\n",
      "Numeric features: 6\n",
      "Categorical features: 13\n",
      "\n",
      "==== CREATING VISUALIZATIONS ====\n",
      "Visualizations created and saved in 'visualizations' folder\n",
      "\n",
      "==== BUILDING AND TRAINING MODELS ====\n",
      "\n",
      "Training SGD Linear Regression...\n",
      "SGD Linear Regression - Train MSE: 4.58, Test MSE: 3.10\n",
      "SGD Linear Regression - Train MAE: 0.49, Test MAE: 0.44\n",
      "SGD Linear Regression - R² Score: 0.77\n",
      "\n",
      "Training Linear Regression...\n",
      "Linear Regression - Train MSE: 4.57, Test MSE: 3.11\n",
      "Linear Regression - Train MAE: 0.51, Test MAE: 0.45\n",
      "Linear Regression - R² Score: 0.77\n",
      "\n",
      "Training Decision Tree...\n",
      "Decision Tree - Train MSE: 0.00, Test MSE: 14.32\n",
      "Decision Tree - Train MAE: 0.00, Test MAE: 1.84\n",
      "Decision Tree - R² Score: -0.04\n",
      "\n",
      "Training Random Forest...\n",
      "Random Forest - Train MSE: 0.82, Test MSE: 4.50\n",
      "Random Forest - Train MAE: 0.44, Test MAE: 1.12\n",
      "Random Forest - R² Score: 0.67\n",
      "\n",
      "Best model based on test MSE: SGD Linear Regression\n",
      "Test MSE: 3.10\n",
      "Test MAE: 0.44\n",
      "R² Score: 0.77\n",
      "\n",
      "==== VISUALIZING LINEAR REGRESSION ====\n",
      "Linear regression visualizations saved in 'visualizations' folder\n",
      "\n",
      "==== TUNING SGD Linear Regression ====\n",
      "Starting grid search (this may take some time)...\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Best parameters: {'regressor__alpha': 0.0001, 'regressor__eta0': 0.01, 'regressor__learning_rate': 'adaptive', 'regressor__max_iter': 500}\n",
      "Tuned model - Train MSE: 4.57, Test MSE: 3.11, R² Score: 0.77\n",
      "\n",
      "==== PREDICTION ON SINGLE TEST EXAMPLE ====\n",
      "Example features:\n",
      "  Hours_Studied: 20\n",
      "  Attendance: 71\n",
      "  Parental_Involvement: Medium\n",
      "  Access_to_Resources: Low\n",
      "  Extracurricular_Activities: No\n",
      "  Sleep_Hours: 7\n",
      "  Previous_Scores: 87\n",
      "  Motivation_Level: High\n",
      "  Internet_Access: Yes\n",
      "  Tutoring_Sessions: 1\n",
      "  Family_Income: Medium\n",
      "  Teacher_Quality: Medium\n",
      "  School_Type: Public\n",
      "  Peer_Influence: Negative\n",
      "  Physical_Activity: 5\n",
      "  Learning_Disabilities: No\n",
      "  Parental_Education_Level: High School\n",
      "  Distance_from_Home: Near\n",
      "  Gender: Male\n",
      "\n",
      "Actual Exam_Score: 65\n",
      "Predicted Exam_Score: 64.54\n",
      "Absolute error: 0.46\n",
      "\n",
      "Feature importance not available for this model type\n",
      "\n",
      "==== SAVING BEST MODEL ====\n",
      "Best model saved as './models/best_student_performance_model.pkl'\n",
      "\n",
      "==== MODEL TRAINING COMPLETE ====\n",
      "Best model: Tuned SGD Linear Regression\n",
      "Test MSE: 3.11\n",
      "R² Score: 0.77\n",
      "\n",
      "All visualizations saved in 'visualizations' folder\n",
      "Model saved in 'models' folder\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"==== STUDENT PERFORMANCE PREDICTION MODEL ====\")\n",
    "    \n",
    "    # Load data directly from CSV\n",
    "    print(\"Loading data from dataset file\")\n",
    "    df = pd.read_csv('studentPerformanceFactors.csv')\n",
    "    \n",
    "    # Set target column\n",
    "    target_column = 'Exam_Score'\n",
    "    \n",
    "    # Analyze and preprocess data\n",
    "    df, numeric_features, categorical_features = analyze_and_preprocess(df, target_column)\n",
    "    \n",
    "    # Create visualizations\n",
    "    create_visualizations(df, numeric_features, categorical_features, target_column)\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X = df.drop(target_column, axis=1)\n",
    "    y = df[target_column]\n",
    "    \n",
    "    # Build and train models\n",
    "    models, results, best_model_name = build_and_train_models(X, y, numeric_features, categorical_features)\n",
    "        \n",
    "    # Create train/test split for visualizations\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Visualize linear regression results\n",
    "    visualize_linear_regression(models, X, y, numeric_features, target_column)\n",
    "    \n",
    "    # Tune the best model\n",
    "    best_model, test_mse, r2 = tune_best_model(best_model_name, X, y, numeric_features, categorical_features)\n",
    "    \n",
    "    # Make prediction on a single test data point\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    if len(X_test) > 0:\n",
    "        # Select a single test example\n",
    "        single_example = X_test.iloc[0:1]\n",
    "        actual_value = y_test.iloc[0]\n",
    "        \n",
    "        # Make prediction using the best model\n",
    "        predicted_value = best_model.predict(single_example)[0]\n",
    "        \n",
    "        print(f\"\\n==== PREDICTION ON SINGLE TEST EXAMPLE ====\")\n",
    "        print(f\"Example features:\")\n",
    "        for col in single_example.columns:\n",
    "            print(f\"  {col}: {single_example[col].iloc[0]}\")\n",
    "            \n",
    "        print(f\"\\nActual {target_column}: {actual_value}\")\n",
    "        print(f\"Predicted {target_column}: {predicted_value:.2f}\")\n",
    "        print(f\"Absolute error: {abs(actual_value - predicted_value):.2f}\")\n",
    "    \n",
    "    # Analyze feature importance for the best model\n",
    "    feature_importance(best_model, numeric_features, categorical_features)\n",
    "    \n",
    "    # Save the best model\n",
    "    save_best_model(best_model, numeric_features, categorical_features, target_column)\n",
    "    \n",
    "    print(\"\\n==== MODEL TRAINING COMPLETE ====\")\n",
    "    print(f\"Best model: Tuned {best_model_name}\")\n",
    "    print(f\"Test MSE: {test_mse:.2f}\")\n",
    "    print(f\"R² Score: {r2:.2f}\")\n",
    "    print(\"\\nAll visualizations saved in 'visualizations' folder\")\n",
    "    print(\"Model saved in 'models' folder\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
